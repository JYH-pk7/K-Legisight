"""
inference_test.py
"""
import json
import torch
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from tqdm import tqdm
import os 
import sys

# =========================================
# 1. ì„¤ì •ê°’ ë° ëª¨ë¸ ë¡œë“œ (ê²½ë¡œ ìˆ˜ì •ë¨!)
# =========================================

if len(sys.argv) < 3:
    print("ğŸš¨ ì˜¤ë¥˜: ì…ë ¥ ë° ì¶œë ¥ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("ì‚¬ìš©ë²•: python ìŠ¤í¬ë¦½íŠ¸ëª….py <ì…ë ¥_JSON_ê²½ë¡œ> <ì¶œë ¥_JSON_ê²½ë¡œ>")
    # ì˜ˆì‹œ: python inference_test.py ./data/speeches_1.json ./output/result_1.json
    sys.exit(1)

# ì¸ìˆ˜ë¡œ ë°›ì€ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— í• ë‹¹ (ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í• ë‹¹ sys[0]:ì‹¤í–‰íŒŒì¼, sys[1]:ì¸í’‹íŒŒì¼, sys[2]:ì•„ì›ƒí’‹íŒŒì¼)
INPUT_JSON_FILE = sys.argv[1] 
OUTPUT_JSON_FILE = sys.argv[2] 
# ---------------------------------------------------------------> python (inference_test.py) (./data/speeches_50176.json) (./division_out/result_50176_3.json)

# MODEL_PATH = "./sentiment_analysis/models/bert_sentiment_v1" 
MODEL_PATH = "./sentiment_analysis/models/hufgging_ji_sentiment_v2" 
# MODEL_PATH = "./sentiment_analysis/models/monologg_kobert_sentiment_v3" 

LABELS = ["í˜‘ë ¥", "ì¤‘ë¦½", "ë¹„í˜‘ë ¥"] # ë¼ë²¨ ë§¤í•‘ (0, 1, 2)

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
try:
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_PATH})")

    # ğŸš¨ğŸš¨ğŸš¨ trust_remote_code=True ì˜µì…˜ ì¶”ê°€ ğŸš¨ğŸš¨ğŸš¨
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True, trust_remote_code=True)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, local_files_only=True, trust_remote_code=True)
    model.eval()

except Exception as e:
    print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"ê²½ë¡œ: {MODEL_PATH}ì— ëª¨ë¸ íŒŒì¼ì´ ì œëŒ€ë¡œ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ì‚¬ìš© ì¥ì¹˜: {device}")


# =========================================
# 2. ì˜ˆì¸¡ ë° í™•ë¥ ê°’ ì¶”ì¶œ í•¨ìˆ˜
# =========================================
def get_sentiment_probabilities(text: str):
    """
    í•˜ë‚˜ì˜ ë°œí™” í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ 3ê°€ì§€ ê°ì„± í™•ë¥ ê°’ê³¼ ìµœì¢… ì˜ˆì¸¡ ë¼ë²¨ì„ ë°˜í™˜
    """
    if not text.strip():
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        return {
            "prediction": "ì¤‘ë¦½",
            "probabilities": {"í˜‘ë ¥": 0.0, "ì¤‘ë¦½": 1.0, "ë¹„í˜‘ë ¥": 0.0}
        }
    
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    probabilities = torch.softmax(logits, dim=1).squeeze().tolist() 

    predicted_index = np.argmax(probabilities)
    predicted_label = LABELS[predicted_index]
    
    return {
        "prediction": predicted_label,
        "probabilities": {label: prob for label, prob in zip(LABELS, probabilities)}
    }


# =========================================
# 3. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =========================================
def run_inference_pipeline():
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (division_out)
    output_dir = os.path.dirname(OUTPUT_JSON_FILE)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")

    print(f"ğŸ“¥ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {INPUT_JSON_FILE}")
    try:
        # JSON íŒŒì¼ì„ ë¡œë“œí•  ë•Œ, C:\Project\K-Legisightë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ë§ì¶°ì•¼ í•¨
        with open(INPUT_JSON_FILE, 'r', encoding='utf-8') as f:
            speeches = json.load(f)
    except FileNotFoundError:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš°, í˜„ì¬ ì‹¤í–‰ ìœ„ì¹˜ì™€ ê²½ë¡œë¥¼ í™•ì¸í•˜ë¼ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
        print(f"ğŸš¨ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ê¸°ëŒ€í•œ ê²½ë¡œ: {os.path.abspath(INPUT_JSON_FILE)}")
        print("ì‹¤í–‰ ìœ„ì¹˜(Current Working Directory)ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ê²½ë¡œë¥¼ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.")
        return
    except json.JSONDecodeError:
        print("ğŸš¨ JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"ì´ ë°œí™” ìˆ˜: {len(speeches)}ê°œ. ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    processed_speeches = []
    # tqdmì„ ì‚¬ìš©í•´ ì§„í–‰ë¥  í‘œì‹œ
    for speech in tqdm(speeches, desc="Sentiment Analysis"):
        speech_text = speech.get("speech_text", "")
        
        # ê°ì„± ë¶„ì„ ìˆ˜í–‰
        sentiment_data = get_sentiment_probabilities(speech_text)
        
        # ì›ë³¸ JSON ë°ì´í„°ì— ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
        speech_new = dict(speech)
        speech_new["sentiment_result"] = {
            "predicted_label": sentiment_data["prediction"],
            "probabilities": sentiment_data["probabilities"]
        }
        
        processed_speeches.append(speech_new)
        
    print(f"\nâœ… ê°ì„± ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ {OUTPUT_JSON_FILE}ì— ì €ì¥í•©ë‹ˆë‹¤.")

    # ê²°ê³¼ JSON ì €ì¥
    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(processed_speeches, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    run_inference_pipeline()