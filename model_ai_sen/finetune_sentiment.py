'''
finetune_sentiment.py
'''
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import sys 

# =========================================
# 1. ì„¤ì •ê°’ ì •ì˜
# =========================================
# MODEL_CHECKPOINT = "klue/bert-base" 
# MODEL_CHECKPOINT = "JiyoungP/QOD-Korean-Political-Sentiment-BERT"
# MODEL_CHECKPOINT = "monologg/kobert" 
MODEL_CHECKPOINT = "monologg/KoELECTRA"

NUM_LABELS = 3

# ì €ì¥ í´ë”
OUTPUT_DIR = "./sentiment_analysis/models/koelectra_v4"

# ë¼ë²¨ ë§¤í•‘: ëª¨ë¸ ì¶œë ¥ê³¼ ì‚¬ëŒì´ ì½ëŠ” ë¼ë²¨ì„ ì—°ê²°
LABEL_MAP = {
    0: "í˜‘ë ¥", 
    1: "ì¤‘ë¦½", 
    2: "ë¹„í˜‘ë ¥"
}


# =========================================
# 2. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (í† í°í™”)
# =========================================
def load_and_tokenize_data(tokenizer):
    """
    í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í† í¬ë‚˜ì´ì§•í•˜ì—¬ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    ë°ì´í„°ì…‹ íŒŒì¼(train.csv, validation.csv)ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        tokenizer (transformers.PreTrainedTokenizer): ê°ê°ì˜ ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ê°ì²´.
    """    
    try:
        df_train = pd.read_csv("./sentiment_analysis/data/train.csv")
        df_valid = pd.read_csv("./sentiment_analysis/data/validation.csv")
    except FileNotFoundError:
        print("ğŸš¨ ì˜¤ë¥˜: data/train.csv ë˜ëŠ” data/validation.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë°ì´í„° íŒŒì¼ì„ data í´ë”ì— ë„£ê³  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None

    # Pandas DataFrameì„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
    raw_datasets = DatasetDict({
        "train": Dataset.from_pandas(df_train),
        "validation": Dataset.from_pandas(df_valid)
    })
    
    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„: 'speech_text', ë¼ë²¨ ì»¬ëŸ¼ ì´ë¦„: 'label' ì´ë¼ê³  ê°€ì •
    def tokenize_function(examples):
        # ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ (truncation), íŒ¨ë”©(padding) ì ìš©
        return tokenizer(examples["speech_text"], truncation=True, padding="max_length")

    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë§Œ ì œê±°í•˜ê³ , ì—†ëŠ” __index_level_0__ëŠ” ì œê±° ëª©ë¡ì—ì„œ ì œì™¸
    tokenized_datasets = tokenized_datasets.remove_columns(["speech_text"])
    tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    tokenized_datasets.set_format("torch")
    
    return tokenized_datasets


# =========================================
# 3. í‰ê°€ ì§€í‘œ ì •ì˜
# =========================================
def compute_metrics(p):
    # pëŠ” (predictions, label_ids) íŠœí”Œ
    preds = np.argmax(p.predictions, axis=1) # ê°€ì¥ ë†’ì€ ë¡œì§“ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ
    
    # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì—ì„œ F1-scoreëŠ” 'weighted'ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
    return {
        "accuracy": accuracy_score(p.label_ids, preds),
        "f1_weighted": f1_score(p.label_ids, preds, average="weighted"),
        "f1_macro": f1_score(p.label_ids, preds, average="macro"),
    }


# =========================================
# 4. ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ ì‹¤í–‰
# í•™ìŠµ ì™„ë£Œ í›„, ìµœì¢… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ OUTPUT_DIRì— ì €ì¥
# =========================================
def run_finetuning():
############### ì´ê±´ klue, hugging ###################
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT) # â˜…â˜…â˜… ì´ í•œ ì¤„ë¡œ ë³€ê²½ â˜…â˜…â˜…    # ë°ì´í„° ì¤€ë¹„
    tokenized_datasets = load_and_tokenize_data(tokenizer)
    if tokenized_datasets is None:
        return

    # ëª¨ë¸ ë¡œë“œ: num_labels=3 ì´ í•µì‹¬!
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT, 
        num_labels=NUM_LABELS
    )
#####################################################
    # tokenizer = AutoTokenizer.from_pretrained(
    #     MODEL_CHECKPOINT,
    #     trust_remote_code=True 
    # )

    # tokenized_datasets = load_and_tokenize_data(tokenizer) # <--- ì—¬ê¸°ê°€ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨
    # if tokenized_datasets is None:
    #     return
    
    # # 2. ëª¨ë¸ ë¡œë“œ ì‹œì—ë„ ë™ì¼í•˜ê²Œ ì ìš©
    # model = AutoModelForSequenceClassification.from_pretrained(
    #     MODEL_CHECKPOINT,
    #     num_labels=NUM_LABELS,
    #     trust_remote_code=True 
    # )
#############################################################
    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        weight_decay=0.01,
        eval_strategy="epoch", # ë§¤ ì—í¬í¬ë§ˆë‹¤ ê²€ì¦
        save_strategy="epoch",       # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì €ì¥
        load_best_model_at_end=True, # í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì  ëª¨ë¸ ë¡œë“œ
        metric_for_best_model="f1_weighted", # ìµœì  ëª¨ë¸ ì„ ì • ê¸°ì¤€
    )

# Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        # tokenizer=tokenizer, # â˜…â˜…â˜… ì´ ì¤„ì€ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œë˜ì–´ ìˆì–´ì•¼ í•¨ (OK)
        compute_metrics=compute_metrics,
    )

    print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
    trainer.train()
    
    # 1. ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (Trainerê°€ ì €ì¥)
    trainer.save_model(OUTPUT_DIR)
    
    # 2. í† í¬ë‚˜ì´ì € ì €ì¥ (KoBERT ë²„ê·¸ ìš°íšŒë¥¼ ìœ„í•œ try-except ë¸”ë¡)
    try:
        # í‘œì¤€ ë°©ì‹ìœ¼ë¡œ ë¨¼ì € ì‹œë„ (ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì— ìœ íš¨)
        tokenizer.save_pretrained(OUTPUT_DIR) 
        
    except TypeError as e:
        # TypeError ë°œìƒ ì‹œ (KoBERTì˜ filename_prefix ë¬¸ì œ), ìˆ˜ë™ ë³µêµ¬ ë¡œì§ ì‹¤í–‰
        if "filename_prefix" in str(e):
            print("âš ï¸ KoBERT í† í¬ë‚˜ì´ì € ì €ì¥ ì˜¤ë¥˜ ë°œìƒ: 'filename_prefix' ë¬¸ì œ. ìˆ˜ë™ ë³µêµ¬ ë¡œì§ ì‹¤í–‰.")
            
            # í† í¬ë‚˜ì´ì €ì˜ íŒŒì¼ë“¤ (vocab.txt ë“±)ë§Œ ìˆ˜ë™ìœ¼ë¡œ ì €ì¥
            vocab_files = tokenizer.save_vocabulary(OUTPUT_DIR)
            print(f"   -> í•„ìˆ˜ Vocab íŒŒì¼ë“¤ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {vocab_files}")
        else:
            # ì˜ˆìƒì¹˜ ëª»í•œ ë‹¤ë¥¸ TypeErrorë©´ ê·¸ëƒ¥ ë‹¤ì‹œ ë°œìƒì‹œí‚´
            raise e
        
    # 3. ëª¨ë¸ ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€
    print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_finetuning()