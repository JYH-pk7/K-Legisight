===================
ëª¨ë¸ ì„±ëŠ¥ì§€í‘œ ì¶œë ¥
===================

import json
import torch
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, ElectraTokenizerFast, BertTokenizerFast
from tqdm import tqdm
import sys
import os 
import pandas as pd

# =========================================
# 1. ì„¤ì •ê°’ ë° ëª¨ë¸ ë¡œë“œ (ê²½ë¡œ ì¼ë°˜í™”)
# =========================================
# â˜…â˜…â˜… í•„ìˆ˜ í™•ì¸: Test JSON íŒŒì¼ ê²½ë¡œë¥¼ ì¸ìˆ˜ë¡œ ë°›ìŒ â˜…â˜…â˜…
if len(sys.argv) < 2:
    print("ğŸš¨ ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("ì‚¬ìš©ë²•: python evaluation_test.py <í…ŒìŠ¤íŠ¸_JSON_ê²½ë¡œ>")
    sys.exit(1)

TEST_JSON_FILE = sys.argv[1]
LABELS = ["í˜‘ë ¥", "ì¤‘ë¦½", "ë¹„í˜‘ë ¥"]
LABEL_TO_ID = {label: i for i, label in enumerate(LABELS)}

BASE_MODEL_DIR = "./sentiment_analysis/models/kobert_v3/" 

# ì œì¼ ì¢‹ì€ ì²´í¬í¬ì¸íŠ¸ì˜ ê°€ì¤‘ì¹˜ ì ìš© 
FINE_TUNED_MODEL_DIR = "./sentiment_analysis/models/kobert_v3/checkpoint-18"
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUê°€ ì•ˆë˜ë©´ ìë™ìœ¼ë¡œ CPU ì‚¬ìš©)
try:
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘... ({FINE_TUNED_MODEL_DIR})")
    
    # ğŸš¨ í† í¬ë‚˜ì´ì €ëŠ” ë² ì´ìŠ¤ ê²½ë¡œì—ì„œ ë¡œë“œ ğŸš¨
    tokenizer = BertTokenizerFast.from_pretrained(
        BASE_MODEL_DIR, 
        local_files_only=True,      
        trust_remote_code=True      
    )

    # ğŸš¨ ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ë¡œë“œ ğŸš¨
    model = AutoModelForSequenceClassification.from_pretrained(
        FINE_TUNED_MODEL_DIR,
        local_files_only=True,
        trust_remote_code=True
    )
    model.eval()
except Exception as e:
    print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"ê²½ë¡œ: {FINE_TUNED_MODEL_DIR}ì— ëª¨ë¸ íŒŒì¼ì´ ì œëŒ€ë¡œ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ì‚¬ìš© ì¥ì¹˜: {device}")


# =========================================
# 2. ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜
# =========================================
def run_evaluation():
    # 1. ë°ì´í„° ë¡œë“œ (Test set)
    print(f"ğŸ“¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {TEST_JSON_FILE}")
    try:
        # ğŸš¨ğŸš¨ğŸš¨ json.load ëŒ€ì‹  pandas.read_csv ì‚¬ìš© ğŸš¨ğŸš¨ğŸš¨
        # íŒŒì¼ ì´ë¦„ì´ .csvì´ë¯€ë¡œ pandasë¡œ ì½ì–´ì•¼ í•¨
        test_df = pd.read_csv(TEST_JSON_FILE)
        
        # DataFrameì„ ë¦¬ìŠ¤íŠ¸ ì˜¤ë¸Œì íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„ ìœ„í•´)
        test_data = test_df.to_dict('records') 
        
    except FileNotFoundError:
        print(f"ğŸš¨ ì˜¤ë¥˜: {TEST_JSON_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ ë¡œë“œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    
    true_labels = [] # ì‹¤ì œ ì •ë‹µ ë¼ë²¨
    predictions = [] # ëª¨ë¸ ì˜ˆì¸¡ ë¼ë²¨
    
    print(f"ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(test_data)}ê°œ. ì˜ˆì¸¡ ì‹œì‘...")
    
    # 2. ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
    for item in tqdm(test_data, desc="Evaluating Model"):
        text = item.get("speech_text", "")
        
        # ğŸš¨ğŸš¨ğŸš¨ CSVì—ì„œ ìˆ«ìë¡œ ì½ì–´ì˜¨ 'label' ê°’ì„ ê°€ì ¸ì™€ ë¬¸ìì—´ë¡œ ë³€í™˜ ğŸš¨ğŸš¨ğŸš¨
        true_label_int = item.get("label", -1) # ìˆ«ìë¡œ ê°€ì ¸ì˜´
        
        # LABELS ë¦¬ìŠ¤íŠ¸ì— ë§ê²Œ ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        if true_label_int in [0, 1, 2]:
            true_label_str = LABELS[true_label_int] # 0:'í˜‘ë ¥', 1:'ì¤‘ë¦½', 2:'ë¹„í˜‘ë ¥'ìœ¼ë¡œ ë³€í™˜
        else:
            true_label_str = "" # ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì€ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬

        if not text.strip() or true_label_str not in LABELS:
            continue

        # ì´ì œ true_labels.append(LABEL_TO_ID[true_label_str]) ì½”ë“œê°€ ì •ìƒ ì‘ë™í•¨!
        true_labels.append(LABEL_TO_ID[true_label_str])
        
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
        
        predicted_index = torch.argmax(logits, dim=1).item()
        predictions.append(predicted_index)

    # 3. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    if not true_labels:
        print("ğŸš¨ ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    accuracy = accuracy_score(true_labels, predictions)
    f1_weighted = f1_score(true_labels, predictions, average='weighted', zero_division=0)
    f1_macro = f1_score(true_labels, predictions, average='macro', zero_division=0)
    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)

    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("           ğŸ¤– ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ğŸ¤–")
    print("="*50)
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(true_labels)}ê°œ")
    print(f"ì •í™•ë„ (Accuracy): {accuracy:.4f}")
    print(f"F1-Score (Weighted): {f1_weighted:.4f}")
    print(f"F1-Score (Macro): {f1_macro:.4f}")
    print(f"ì •ë°€ë„ (Precision): {precision:.4f}")
    print(f"ì¬í˜„ìœ¨ (Recall): {recall:.4f}")
    print("="*50)


if __name__ == "__main__":

    run_evaluation()
