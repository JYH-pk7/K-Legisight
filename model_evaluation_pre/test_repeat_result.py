#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
test_repeat_result.py
------------------------
LLM ê¸°ë°˜ trigger_deliber_x.py ê²°ê³¼ë¥¼ ë™ì¼ íšŒì˜ë¡ì— ëŒ€í•´ NíšŒ ë°˜ë³µ ìˆ˜í–‰í•˜ê³ 
ê° runë§ˆë‹¤ compare_segments.pyë¡œ í‰ê°€í•˜ì—¬ í‰ê·  / ë¶„ì‚° / ìµœê³  ì„±ëŠ¥ run ê³„ì‚°
ê²°ê³¼ë¥¼ JSON + TXT ë³´ê³ ì„œë¡œ ì €ì¥
ì‹¤í–‰ ì»¤ë§¨ë“œ python test_repeat_result.py --committee (ì´ë¦„) --meeting (ë²ˆí˜¸) --repeat 10 
"""

import sys, os
import json
import subprocess
import argparse
import statistics
import importlib   # âœ… ì¶”ê°€

# âœ… ìƒìœ„ í´ë” import ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model_evaluation_pre.compare_segments import compare_meeting


def main():
    parser = argparse.ArgumentParser(description="ë°˜ë³µ í…ŒìŠ¤íŠ¸ ë° ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±")
    parser.add_argument("--committee", required=True)
    parser.add_argument("--meeting", type=int, required=True)
    parser.add_argument("--repeat", type=int, default=10)
    args = parser.parse_args()

    committee = args.committee.strip().lower()
    meeting_id = args.meeting
    n_repeat = args.repeat

    print(f"\nğŸš€ {committee} ìœ„ì›íšŒ íšŒì˜({meeting_id}) ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({n_repeat}íšŒ ë°˜ë³µ)\n")

    # âœ… ë™ì ìœ¼ë¡œ ì •ë‹µ ì„¸íŠ¸ import
    try:
        mod = importlib.import_module(f"model_evaluation_pre.answers.answers_{committee}")
        answer_segments = mod.answer_segments
    except ModuleNotFoundError:
        print(f"âš ï¸ ì •ë‹µì„¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: answers_{committee}.py")
        return

    # âœ… ê²½ë¡œ ì„¤ì •
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    trigger_path = os.path.join(base_dir, "preprocess_model", "trigger_deliber_1.py")
    output_dir = os.path.join(base_dir, "preprocess_model", "trigger_deliber_output")

    if not os.path.exists(trigger_path):
        print(f"âš ï¸ trigger_deliber_1.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {trigger_path}")
        return

    f1_scores, precisions, recalls = [], [], []

    # âœ… ë°˜ë³µ ìˆ˜í–‰
    for i in range(n_repeat):
        print(f"ğŸ” [{i+1}/{n_repeat}] trigger_deliber_1.py ì‹¤í–‰ ì¤‘...")
        subprocess.run(["python", trigger_path],
                       cwd=os.path.join(base_dir, "preprocess_model"),
                       check=True)

        print("âš–ï¸  compare_segments í‰ê°€ ì¤‘...")
        gold_segments = answer_segments.get(meeting_id)
        if not gold_segments:
            print(f"âš ï¸ {committee} ìœ„ì›íšŒ {meeting_id} íšŒì˜ì˜ ì •ë‹µ ì„¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        result = compare_meeting(meeting_id, gold_segments, output_dir=output_dir)
        if result:
            precisions.append(result["precision"])
            recalls.append(result["recall"])
            f1_scores.append(result["f1"])

    # âœ… í†µê³„ ê³„ì‚°
    avg_p = sum(precisions) / len(precisions) if precisions else 0
    avg_r = sum(recalls) / len(recalls) if recalls else 0
    avg_f = sum(f1_scores) / len(f1_scores) if f1_scores else 0
    var_f = statistics.variance(f1_scores) if len(f1_scores) > 1 else 0.0
    std_f = statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0.0

    best_score = max(f1_scores) if f1_scores else 0
    best_runs = [i + 1 for i, f in enumerate(f1_scores) if f == best_score]
    best_rate = len(best_runs) / n_repeat if n_repeat > 0 else 0
    fail_rate = sum(f < 0.5 for f in f1_scores) / n_repeat if n_repeat > 0 else 0
    stability = max(0.0, 1 - std_f)  # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì•ˆì •ì 

    # âœ… ì½˜ì†” ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ë°˜ë³µ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ì´ ë°˜ë³µ íšŸìˆ˜     : {n_repeat}")
    print(f"í‰ê·  Precision   : {avg_p:.3f}")
    print(f"í‰ê·  Recall      : {avg_r:.3f}")
    print(f"í‰ê·  F1-score    : {avg_f:.3f}")
    print(f"F1-score ë¶„ì‚°    : {var_f:.5f}")
    print(f"í‘œì¤€í¸ì°¨(Ïƒ)      : {std_f:.5f}")
    print(f"ìµœê³  ì„±ëŠ¥ Run    : {', '.join('#'+str(r) for r in best_runs)} (F1={best_score:.3f})")
    print(f"Best Run í™•ë¥     : {best_rate:.2f} ({len(best_runs)}/{n_repeat}íšŒ)")
    print(f"Fail Run ë¹„ìœ¨    : {fail_rate:.2f} (F1<0.5)")
    print(f"ëª¨ë¸ ì•ˆì •ë„(1âˆ’Ïƒ) : {stability:.2f}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    # âœ… ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    report_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "reports")
    os.makedirs(report_dir, exist_ok=True)

    # âœ… JSON ê²°ê³¼ ì €ì¥
    json_path = os.path.join(report_dir, f"repeat_result_{committee}_{meeting_id}.json")
    result_data = {
        "committee": committee,
        "meeting_id": meeting_id,
        "repeat": n_repeat,
        "avg_precision": avg_p,
        "avg_recall": avg_r,
        "avg_f1": avg_f,
        "variance_f1": var_f,
        "stdev_f1": std_f,
        "best_f1": best_score,
        "best_runs": best_runs,
        "best_rate": best_rate,
        "fail_rate": fail_rate,
        "stability": stability,
        "f1_scores": f1_scores,
    }
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    # âœ… TXT ë³´ê³ ì„œ ì €ì¥
    txt_path = os.path.join(report_dir, f"repeat_summary_{committee}_{meeting_id}.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(f"ğŸ“„ {committee.upper()} ìœ„ì›íšŒ íšŒì˜ {meeting_id} ë°˜ë³µ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
        f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
        f.write(f"ì´ ë°˜ë³µ íšŸìˆ˜     : {n_repeat}\n")
        f.write(f"í‰ê·  Precision   : {avg_p:.3f}\n")
        f.write(f"í‰ê·  Recall      : {avg_r:.3f}\n")
        f.write(f"í‰ê·  F1-score    : {avg_f:.3f}\n")
        f.write(f"F1-score ë¶„ì‚°    : {var_f:.5f}\n")
        f.write(f"í‘œì¤€í¸ì°¨(Ïƒ)      : {std_f:.5f}\n")
        f.write(f"ìµœê³  ì„±ëŠ¥ Run    : {', '.join('#'+str(r) for r in best_runs)} (F1={best_score:.3f})\n")
        f.write(f"Best Run í™•ë¥     : {best_rate:.2f} ({len(best_runs)}/{n_repeat}íšŒ)\n")
        f.write(f"Fail Run ë¹„ìœ¨    : {fail_rate:.2f} (F1<0.5)\n")
        f.write(f"ëª¨ë¸ ì•ˆì •ë„(1âˆ’Ïƒ) : {stability:.2f}\n")
        f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n")

    # âœ… ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ì¡° í‰ê°€ ì§€í‘œ (ì˜µì…˜)
        f.write("ğŸ“ˆ ë³´ì¡° í‰ê°€ ì§€í‘œ (ì°¸ê³ ìš©)\n")
        f.write("- Best Run í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ ì¼ê´€ëœ íŒ¨í„´ì„ ë³´ì„\n")
        f.write("- Fail Run ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì˜¨ë„ë‚˜ LLM randomness ì¡°ì • í•„ìš”\n")
        f.write("- ëª¨ë¸ ì•ˆì •ë„(1âˆ’Ïƒ)ëŠ” 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê²°ê³¼ê°€ ì¼ì •í•¨\n")
        f.write("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n")

        f.write("Runë³„ F1-score:\n")
        for i, f1 in enumerate(f1_scores, 1):
            bar = "â–ˆ" * int(f1 * 20)
            f.write(f"  Run {i:2d}: {f1:.3f} | {bar}\n")

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {json_path}")
    print(f"ğŸ“ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥ â†’ {txt_path}")


if __name__ == "__main__":
    main()